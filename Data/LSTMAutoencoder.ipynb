{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Num GPUs Available:  0\n"
    }
   ],
   "source": [
    "#stupid jupyter path is in the venv directory, so this is in MTG-INVESTOR-S-GRAIL\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.utils import Sequence\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import Data.sadness as sadness\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "%matplotlib inline\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making file with text from just standard cards\n",
    "dftemp1 = pd.read_csv('Data/LSTMData.csv', sep=',')\n",
    "dftemp1 = dftemp1[pd.notnull(dftemp1['oracle_text'])]\n",
    "with open('Data/LSTMData.txt', 'w', encoding='UTF-8') as fileboi:\n",
    "    for tupleboi in dftemp1.itertuples():\n",
    "        if str(tupleboi[2]).count('nan') == 0:\n",
    "            fileboi.write(str(tupleboi[2])+ '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2jank = 'Data/LSTMData.txt'\n",
    "janktext = open(path2jank, 'rb').read().decode(encoding='utf-8')\n",
    "vocablist = sorted(set(janktext).union(set(sadness.safe_string)))\n",
    "num_chars = len(vocablist)\n",
    "char2idx = {u:i for i, u in enumerate(vocablist)}\n",
    "idx2char = {value:key for key, value in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "80 unique characters\n  '\\n':   0,\n  '\\r':   1,\n  ' ' :   2,\n  '\"' :   3,\n  \"'\" :   4,\n  '(' :   5,\n  ')' :   6,\n  '+' :   7,\n  ',' :   8,\n  '-' :   9,\n  '.' :  10,\n  '/' :  11,\n  '0' :  12,\n  '1' :  13,\n  '2' :  14,\n  '3' :  15,\n  '4' :  16,\n  '5' :  17,\n  '6' :  18,\n  '7' :  19,\n  '8' :  20,\n  '9' :  21,\n  ':' :  22,\n  'A' :  23,\n  'B' :  24,\n  'C' :  25,\n  'D' :  26,\n  'E' :  27,\n  'F' :  28,\n  'G' :  29,\n  'H' :  30,\n  'I' :  31,\n  'J' :  32,\n  'K' :  33,\n  'L' :  34,\n  'M' :  35,\n  'N' :  36,\n  'O' :  37,\n  'P' :  38,\n  'Q' :  39,\n  'R' :  40,\n  'S' :  41,\n  'T' :  42,\n  'U' :  43,\n  'V' :  44,\n  'W' :  45,\n  'X' :  46,\n  'Y' :  47,\n  'Z' :  48,\n  'a' :  49,\n  'b' :  50,\n  'c' :  51,\n  'd' :  52,\n  'e' :  53,\n  'f' :  54,\n  'g' :  55,\n  'h' :  56,\n  'i' :  57,\n  'j' :  58,\n  'k' :  59,\n  'l' :  60,\n  'm' :  61,\n  'n' :  62,\n  'o' :  63,\n  'p' :  64,\n  'q' :  65,\n  'r' :  66,\n  's' :  67,\n  't' :  68,\n  'u' :  69,\n  'v' :  70,\n  'w' :  71,\n  'x' :  72,\n  'y' :  73,\n  'z' :  74,\n  '{' :  75,\n  '}' :  76,\n  '—' :  77,\n  '•' :  78,\n  '−' :  79,\n"
    }
   ],
   "source": [
    "print ('{} unique characters'.format(num_chars))\n",
    "for char,_ in zip(char2idx, range(num_chars)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for tupley in dftemp1.itertuples():\n",
    "    temparray = np.zeros((len(tupley[2]), num_chars))\n",
    "    temparray[[range(len(tupley[2]))], [char2idx[c] for c in tupley[2]]] = 1\n",
    "    X_train.append(temparray)\n",
    "Y_train = X_train\n",
    "class MyBatchGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, X, y, batch_size=1, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.y)/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__data_generation(index)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Shuffles indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.y))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        Xb = np.zeros((self.batch_size, *self.X[index].shape))\n",
    "        yb = np.zeros((self.batch_size, *self.y[index].shape))\n",
    "        # naively use the same sample over and over again\n",
    "        for s in range(0, self.batch_size):\n",
    "            Xb[s] = self.X[index]\n",
    "            yb[s] = self.y[index]\n",
    "        return Xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(num_chars, input_shape=(None, num_chars), return_sequences = True))\n",
    "model.add(tf.keras.layers.LSTM(50, return_sequences = True))\n",
    "model.add(tf.keras.layers.LSTM(20, return_sequences = True))\n",
    "model.add(tf.keras.layers.LSTM(50, return_sequences = True))\n",
    "model.add(tf.keras.layers.LSTM(num_chars, return_sequences = True))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_5 (LSTM)                (None, None, 80)          51520     \n_________________________________________________________________\nlstm_6 (LSTM)                (None, None, 50)          26200     \n_________________________________________________________________\nlstm_7 (LSTM)                (None, None, 20)          5680      \n_________________________________________________________________\nlstm_8 (LSTM)                (None, None, 50)          14200     \n_________________________________________________________________\nlstm_9 (LSTM)                (None, None, 80)          41920     \n=================================================================\nTotal params: 139,520\nTrainable params: 139,520\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/10\n821/821 [==============================] - 676s 824ms/step - loss: 8.3312\nEpoch 2/10\n566/821 [===================>..........] - ETA: 3:24 - loss: 8.1153"
    }
   ],
   "source": [
    "model.fit_generator(MyBatchGenerator(X_train, Y_train, batch_size=1), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}